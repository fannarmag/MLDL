{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def mnist_fun(args, ctx):\n",
    "    \n",
    "    def print_log(worker_num, fd, arg):\n",
    "        fd.write(\"{} - {}\\n\".format(str(worker_num), str(arg)))\n",
    "\n",
    "    from tensorflowonspark import TFNode\n",
    "    from datetime import datetime\n",
    "    import getpass\n",
    "    import math\n",
    "    import numpy\n",
    "    import os\n",
    "    import signal\n",
    "    import tensorflow as tf\n",
    "    import time\n",
    "  \n",
    "    # Used to get TensorBoard logdir for TensorBoard that show up in HopsWorks\n",
    "    from hops import tensorboard\n",
    "    from hops import hdfs\n",
    "    \n",
    "    # Set up our own log file\n",
    "    logfile_path = hdfs.project_path() + \"Logs/mylogfile.txt\"\n",
    "    fd_log = hdfs.get_fs().open_file(logfile_path, flags='w')\n",
    "    fd_log.write('Logfile for genre classification')\n",
    "\n",
    "    IMAGE_PIXELS=128\n",
    "    worker_num = ctx.worker_num\n",
    "    job_name = ctx.job_name\n",
    "    task_index = ctx.task_index\n",
    "    cluster_spec = ctx.cluster_spec\n",
    "    num_workers = len(cluster_spec['worker'])\n",
    "\n",
    "    # Delay PS nodes a bit, since workers seem to reserve GPUs more quickly/reliably (w/o conflict)\n",
    "    if job_name == \"ps\":\n",
    "        time.sleep((worker_num + 1) * 5)\n",
    "\n",
    "    # Parameters\n",
    "    hidden_units = 128\n",
    "    batch_size   = 100\n",
    "\n",
    "    # Get TF cluster and server instances\n",
    "    cluster, server = TFNode.start_cluster_server(ctx, 1, args.rdma)\n",
    "\n",
    "    def read_tfr_examples(path, batch_size=100, num_epochs=None, task_index=None, num_workers=None):\n",
    "        print_log(worker_num, fd_log, \"num_epochs: {0}\".format(num_epochs))\n",
    "\n",
    "        # Setup queue of TFRecord filenames\n",
    "        tf_record_pattern = os.path.join(path, 'part-*')\n",
    "        \n",
    "        files = tf.gfile.Glob(tf_record_pattern)\n",
    "        queue_name = \"file_queue\"\n",
    "\n",
    "        # split input files across workers, if specified\n",
    "        if task_index is not None and num_workers is not None:\n",
    "            num_files = len(files)\n",
    "            files = files[task_index:num_files:num_workers]\n",
    "            queue_name = \"file_queue_{0}\".format(task_index)\n",
    "\n",
    "        print_log(worker_num, fd_log, \"files: {0}\".format(files))\n",
    "        file_queue = tf.train.string_input_producer(files, shuffle=False, capacity=1000, num_epochs=num_epochs, name=queue_name)\n",
    "\n",
    "        # Setup reader for examples\n",
    "        reader = tf.TFRecordReader(name=\"reader\")\n",
    "        _, serialized = reader.read(file_queue)\n",
    "        feature_def = {'label': tf.FixedLenFeature([6], tf.int64), 'image': tf.FixedLenFeature([16384], tf.int64) }\n",
    "        features = tf.parse_single_example(serialized, feature_def)\n",
    "        norm = tf.constant(255, dtype=tf.float32, shape=(16384,))\n",
    "        image = tf.div(tf.to_float(features['image']), norm)\n",
    "        print_log(worker_num, fd_log, \"image: {0}\".format(image))\n",
    "        label = tf.to_float(features['label'])\n",
    "        print_log(worker_num, fd_log, \"label: {0}\".format(label))\n",
    "\n",
    "        # Return a batch of examples\n",
    "        return tf.train.batch([image,label], batch_size, num_threads=args.readers, name=\"batch\")\n",
    "\n",
    "    if job_name == \"ps\":\n",
    "        server.join()\n",
    "    elif job_name == \"worker\":\n",
    "        # Assigns ops to the local worker by default.\n",
    "        with tf.device(tf.train.replica_device_setter(\n",
    "            worker_device=\"/job:worker/task:%d\" % task_index, cluster=cluster)):\n",
    "            \n",
    "            # Placeholders or QueueRunner/Readers for input data\n",
    "            num_epochs = 1 if args.mode == \"inference\" else None if args.epochs == 0 else args.epochs\n",
    "            index = task_index if args.mode == \"inference\" else None\n",
    "            workers = num_workers if args.mode == \"inference\" else None\n",
    "\n",
    "            # Read input data from TFRecords\n",
    "            if args.format == \"tfr\":\n",
    "                images = TFNode.hdfs_path(ctx, args.images)\n",
    "                x, y_ = read_tfr_examples(images, 100, num_epochs, index, workers)\n",
    "            else:\n",
    "                raise(\"{0} format not supported for tf input mode\".format(args.format))\n",
    "            \n",
    "            # Convnet setup\n",
    "            \n",
    "            # ORIGINAL FROM DEMO\n",
    "            # Variables of the hidden layer\n",
    "            #hid_w = tf.Variable(tf.truncated_normal([IMAGE_PIXELS * IMAGE_PIXELS, hidden_units], stddev=1.0 / IMAGE_PIXELS), name=\"hid_w\")\n",
    "            #hid_b = tf.Variable(tf.zeros([hidden_units]), name=\"hid_b\")\n",
    "            #tf.summary.histogram(\"hidden_weights\", hid_w)\n",
    "\n",
    "            # Variables of the softmax layer\n",
    "            #sm_w = tf.Variable(tf.truncated_normal([hidden_units, 6], stddev=1.0 / math.sqrt(hidden_units)), name=\"sm_w\")\n",
    "            #sm_b = tf.Variable(tf.zeros([6]), name=\"sm_b\")\n",
    "            #tf.summary.histogram(\"softmax_weights\", sm_w)\n",
    "    \n",
    "            #x_img = tf.reshape(x, [-1, IMAGE_PIXELS, IMAGE_PIXELS, 1])\n",
    "            #tf.summary.image(\"x_img\", x_img)\n",
    "\n",
    "            #hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\n",
    "            #hid = tf.nn.relu(hid_lin)\n",
    "\n",
    "            #y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n",
    "            \n",
    "            #global_step = tf.Variable(0)\n",
    "            \n",
    "            # The image data input is a 1-d vector of size 128*128 = 16384\n",
    "            # Reshape to a tensor of the form [batch size, height, width, channel]\n",
    "            x_img = tf.reshape(x, [-1, IMAGE_PIXELS, IMAGE_PIXELS, 1])\n",
    "            tf.summary.image(\"x_img\", x_img) # Save to tensorboard summary\n",
    "            \n",
    "            pkeep = 0.75\n",
    "            \n",
    "            # Weight and bias variables\n",
    "            # Weights initialized with small random values between -0.2 and +0.2\n",
    "            # The weight variables from your convolutional layers should follow 4-dimensional tensors of (patch_height, patch_width, input_channels, output_channels)\n",
    "            # Convolutional layers\n",
    "            CL1W = tf.Variable(tf.truncated_normal([5, 5, 1, 4], stddev=0.1))\n",
    "            CL1B = tf.Variable(tf.zeros([4]))\n",
    "            CL2W = tf.Variable(tf.truncated_normal([5, 5, 4, 8], stddev=0.1))\n",
    "            CL2B = tf.Variable(tf.zeros([8]))\n",
    "            CL3W = tf.Variable(tf.truncated_normal([4, 4, 8, 12], stddev=0.1))\n",
    "            CL3B = tf.Variable(tf.zeros([12]))\n",
    "            # Fully connected layer\n",
    "            FCW = tf.Variable(tf.truncated_normal([32 * 32 * 12, 200], stddev=0.1))\n",
    "            FCB = tf.Variable(tf.zeros([200]))\n",
    "            # Read out layer\n",
    "            ROW = tf.Variable(tf.truncated_normal([200, 6], stddev=0.1))\n",
    "            ROB = tf.Variable(tf.zeros([6]))\n",
    "\n",
    "            # Convolutional and ReLU layers\n",
    "            CL1 = tf.nn.conv2d(x_img, CL1W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            Y1 = tf.nn.relu(CL1 + CL1B)\n",
    "            Y1d = tf.nn.dropout(Y1, pkeep)\n",
    "            CL2 = tf.nn.conv2d(Y1d, CL2W, strides=[1, 2, 2, 1], padding='SAME')\n",
    "            Y2 = tf.nn.relu(CL2 + CL2B)\n",
    "            Y2d = tf.nn.dropout(Y2, pkeep)\n",
    "            CL3 = tf.nn.conv2d(Y2d, CL3W, strides=[1, 2, 2, 1], padding='SAME')\n",
    "            Y3 = tf.nn.relu(CL3 + CL3B)\n",
    "            Y3d = tf.nn.dropout(Y3, pkeep)\n",
    "\n",
    "            # Fully connected layer\n",
    "            # Reshape to vector\n",
    "            # Y3.shape = (?, 7, 7, 12)\n",
    "            #Y3RS = tf.reshape(Y3, [-1, 28 * 28 * 1])\n",
    "            # 128/2/2 = 32\n",
    "            Y3RS = tf.reshape(Y3d, [-1, 32 * 32 * 12])\n",
    "            Y4 = tf.nn.relu(tf.matmul(Y3RS, FCW) + FCB)\n",
    "\n",
    "            # Read out layer\n",
    "            y_logits = tf.matmul(Y4, ROW) + ROB\n",
    "            y = tf.nn.softmax(y_logits)\n",
    "            \n",
    "            \n",
    "\n",
    "            # Loss op and training op\n",
    "            \n",
    "            # ORIGINAL FROM DEMO\n",
    "            #loss = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n",
    "            #tf.summary.scalar(\"loss\", loss)\n",
    "            #train_op = tf.train.AdagradOptimizer(0.01).minimize(loss, global_step=global_step)\n",
    "\n",
    "            # WARNING: This op expects unscaled logits, since it performs a softmax on logits internally for efficiency. Do not call this op with the output of softmax, as it will produce incorrect results.\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=y_logits, labels=y_) # calculate cross-entropy with logits\n",
    "            loss = tf.reduce_mean(cross_entropy)\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            train_op = tf.train.AdamOptimizer(0.005).minimize(loss, global_step=global_step)\n",
    "            \n",
    "            # Test trained model (accuracy op)\n",
    "            label = tf.argmax(y_, 1, name=\"label\") # Actual label\n",
    "            prediction = tf.argmax(y, 1,name=\"prediction\")\n",
    "            correct_prediction = tf.equal(prediction, label)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "            tf.summary.scalar(\"acc\", accuracy)\n",
    "\n",
    "            # Save, merge summaries and initialize variables\n",
    "            saver = tf.train.Saver()\n",
    "            summary_op = tf.summary.merge_all()\n",
    "            init_op = tf.global_variables_initializer()\n",
    "\n",
    "            # Create a \"supervisor\", which oversees the training process and stores model state into HDFS\n",
    "            logdir = tensorboard.logdir()\n",
    "            print(\"tensorflow model path: {0}\".format(logdir))\n",
    "\n",
    "            if job_name == \"worker\" and task_index == 0:\n",
    "                summary_writer = tf.summary.FileWriter(logdir, graph=tf.get_default_graph())\n",
    "\n",
    "            if args.mode == \"train\":\n",
    "                sv = tf.train.Supervisor(is_chief=(task_index == 0),\n",
    "                                       logdir=logdir,\n",
    "                                       init_op=init_op,\n",
    "                                       summary_op=None,\n",
    "                                       summary_writer=None,\n",
    "                                       saver=saver,\n",
    "                                       global_step=global_step,\n",
    "                                       stop_grace_secs=300,\n",
    "                                       save_model_secs=10)\n",
    "            else:\n",
    "                sv = tf.train.Supervisor(is_chief=(task_index == 0),\n",
    "                                       logdir=logdir,\n",
    "                                       summary_op=None,\n",
    "                                       saver=saver,\n",
    "                                       global_step=global_step,\n",
    "                                       stop_grace_secs=300,\n",
    "                                       save_model_secs=0)\n",
    "            output_dir = TFNode.hdfs_path(ctx, args.output)\n",
    "            output_file = tf.gfile.Open(\"{0}/part-{1:05d}\".format(output_dir, worker_num), mode='w')\n",
    "\n",
    "          # The supervisor takes care of session initialization, restoring from\n",
    "          # a checkpoint, and closing when done or an error occurs.\n",
    "    with sv.managed_session(server.target) as sess:\n",
    "        print(\"{0} session ready\".format(datetime.now().isoformat()))\n",
    "\n",
    "        # Loop until the supervisor shuts down or 1000000 steps have completed.\n",
    "        step = 0\n",
    "        count = 0\n",
    "        while not sv.should_stop() and step < args.steps:\n",
    "        # Run a training step asynchronously.\n",
    "        # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n",
    "        # perform *synchronous* training.\n",
    "\n",
    "            # using QueueRunners/Readers\n",
    "            if args.mode == \"train\":\n",
    "                if (step % 100 == 0):\n",
    "                    print(\"{0} step: {1} accuracy: {2}\".format(datetime.now().isoformat(), step, sess.run(accuracy)))\n",
    "                _, summary, step = sess.run([train_op, summary_op, global_step])\n",
    "                if sv.is_chief:\n",
    "                    summary_writer.add_summary(summary, step)\n",
    "            else: # args.mode == \"inference\"\n",
    "                labels, pred, acc = sess.run([label, prediction, accuracy])\n",
    "                #print(\"label: {0}, pred: {1}\".format(labels, pred))\n",
    "                print(\"acc: {0}\".format(acc))\n",
    "                for i in range(len(labels)):\n",
    "                    count += 1\n",
    "                    output_file.write(\"{0} {1}\\n\".format(labels[i], pred[i]))\n",
    "                print(\"count: {0}\".format(count))\n",
    "        \n",
    "\n",
    "        if args.mode == \"inference\":\n",
    "            output_file.close()\n",
    "            # Delay chief worker from shutting down supervisor during inference, since it can load model, start session,\n",
    "            # run inference and request stop before the other workers even start/sync their sessions.\n",
    "        if task_index == 0:\n",
    "            time.sleep(60)\n",
    "\n",
    "        # Ask for all the services to stop.\n",
    "        print(\"{0} stopping supervisor\".format(datetime.now().isoformat()))\n",
    "        sv.stop()\n",
    "        \n",
    "    fd_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from hops import util\n",
    "from hops import hdfs\n",
    "\n",
    "from tensorflowonspark import TFCluster\n",
    "\n",
    "sc = spark.sparkContext\n",
    "num_executors = util.num_executors(spark)\n",
    "num_ps = util.num_param_servers(spark)\n",
    "\n",
    "# hdfs_project_path = \"hdfs:///Projects/genre_classifier_2/\"\n",
    "hdfs_tfrecords_path = \"/Projects/genre_classifier_2/Spectrograms/tfrecords/training\"\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-e\", \"--epochs\", help=\"number of epochs\", type=int, \n",
    "                    default=0)\n",
    "parser.add_argument(\"-f\", \"--format\", help=\"example format: (csv|pickle|tfr)\", choices=[\"csv\",\"pickle\",\"tfr\"], \n",
    "                    default=\"tfr\")\n",
    "parser.add_argument(\"-i\", \"--images\", help=\"HDFS path to images in parallelized format\", \n",
    "                    default= hdfs_tfrecords_path)\n",
    "parser.add_argument(\"-l\", \"--labels\", help=\"HDFS path to MNIST labels in parallelized format\", \n",
    "                    default = hdfs_tfrecords_path)\n",
    "parser.add_argument(\"-m\", \"--model\", help=\"HDFS path to save/load model during train/test\", \n",
    "                    default=\"genre_classification_model\")\n",
    "parser.add_argument(\"-n\", \"--cluster_size\", help=\"number of nodes in the cluster (for Spark Standalone)\", type=int, \n",
    "                    default=num_executors)\n",
    "parser.add_argument(\"-o\", \"--output\", help=\"HDFS path to save test/inference output\", \n",
    "                    default=\"predictions\")\n",
    "parser.add_argument(\"-r\", \"--readers\", help=\"number of reader/enqueue threads\", type=int, \n",
    "                    default=1)\n",
    "parser.add_argument(\"-s\", \"--steps\", help=\"maximum number of steps\", type=int, \n",
    "                    default=1000)\n",
    "parser.add_argument(\"-tb\", \"--tensorboard\", help=\"launch tensorboard process\", action=\"store_true\")\n",
    "parser.add_argument(\"-X\", \"--mode\", help=\"train|inference\", \n",
    "                    default=\"train\")\n",
    "parser.add_argument(\"-c\", \"--rdma\", help=\"use rdma connection\", \n",
    "                    default=False)\n",
    "args = parser.parse_args()\n",
    "print(\"args:\",args)\n",
    "\n",
    "\n",
    "print(\"{0} ===== Start\".format(datetime.now().isoformat()))\n",
    "\n",
    "cluster = TFCluster.run(sc, mnist_fun, args, args.cluster_size, num_ps, args.tensorboard, TFCluster.InputMode.TENSORFLOW)\n",
    "cluster.shutdown()\n",
    "\n",
    "print(\"{0} ===== Stop\".format(datetime.now().isoformat()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
