{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Preprocess images to tfrecords\n",
    "\n",
    "## Adapted from the demo tfos_mnist_preprocessing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from array import array\n",
    "from hops import hdfs\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def toTFExample(image, label):\n",
    "    \"\"\"Serializes an image/label as a TFExample byte string\"\"\"\n",
    "    example = tf.train.Example(\n",
    "      features = tf.train.Features(\n",
    "        feature = {\n",
    "          'label': tf.train.Feature(int64_list=tf.train.Int64List(value=label.astype(\"int64\"))),\n",
    "          'image': tf.train.Feature(int64_list=tf.train.Int64List(value=image.astype(\"int64\")))\n",
    "        }\n",
    "      )\n",
    "    )\n",
    "    return example.SerializeToString()\n",
    "\n",
    "\n",
    "def fromTFExample(bytestr):\n",
    "    \"\"\"Deserializes a TFExample from a byte string\"\"\"\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(bytestr)\n",
    "    return example\n",
    "\n",
    "\n",
    "def readDataToRDD(sc, folder):\n",
    "    # Want an RDD of the form (label, np array)\n",
    "    # sparkContext.binaryFiles(\"hdfs://a-hdfs-path\") returns (a-hdfs-path/part-00000, its content)\n",
    "    rdd = sc.binaryFiles(folder + \"/**/*.png\")\n",
    "    #print(\"RDD raw\")\n",
    "    #print(rdd.take(2))\n",
    "    # extract the label\n",
    "    rdd = rdd.map(lambda x: (extract_label_one_hot(x[0]), x[1]))\n",
    "    #print(\"RDD label raw\")\n",
    "    #print(rdd.take(1))\n",
    "    # convert binary to array\n",
    "    rdd = rdd.map(lambda x: (x[0], np.array(Image.open(io.BytesIO(x[1])))))\n",
    "    #print(\"RDD label array\")\n",
    "    #print(rdd.take(1))\n",
    "    #print(\"RDD array shape\")\n",
    "    #print(((rdd.take(1)[0])[1]).shape)\n",
    "    # The numpy array is of shape 128x128 - all images should be of that size\n",
    "    rdd = rdd.filter(lambda x: x[1].shape == (128, 128))\n",
    "    #print(\"RDD size after filter\")\n",
    "    #print(str(rdd.count()))\n",
    "    # Reshape numpy array to vector (1d)\n",
    "    rdd = rdd.map(lambda x: (x[0], x[1].reshape(x[1].shape[0] * x[1].shape[1])))\n",
    "    #print(\"RDD label reshaped array\")\n",
    "    #print(rdd.take(1))\n",
    "    #print(\"RDD array shape\")\n",
    "    #print(((rdd.take(1)[0])[1]).shape)\n",
    "    \n",
    "    return rdd\n",
    "\n",
    "def extract_label_one_hot(path):\n",
    "    # File names are of the form genre_X_X.png\n",
    "    # Our music genre labels\n",
    "    label_dict = {\n",
    "    'Classical': 0,\n",
    "    'Techno': 1,\n",
    "    'Pop': 2,\n",
    "    'HipHop': 3,\n",
    "    'Metal': 4,\n",
    "    'Rock': 5\n",
    "    }\n",
    "    filename = path.split(\"/\")[len(path.split(\"/\")) - 1]\n",
    "    genre = filename.split(\"_\")[0]\n",
    "    label_val = int(label_dict.get(genre)) # Should never get a label not in the dict\n",
    "    label_one_hot = np.zeros(len(label_dict), dtype=np.uint8)\n",
    "    label_one_hot[label_val] = 1\n",
    "    return label_one_hot\n",
    "    \n",
    "\n",
    "def write_tf_records(sc, input_dir,  output_dir):\n",
    "    rdd = readDataToRDD(sc, input_dir)\n",
    "    tfRDD = rdd.map(lambda x: (bytearray(toTFExample(x[1], x[0])), None))\n",
    "    # requires: --jars tensorflow-hadoop-1.0-SNAPSHOT.jar\n",
    "    #tfRDD.saveAsNewAPIHadoopFile(output_dir, \"org.tensorflow.hadoop.io.TFRecordFileOutputFormat\",\n",
    "    #                             keyClass=\"org.apache.hadoop.io.BytesWritable\",\n",
    "    #                             valueClass=\"org.apache.hadoop.io.NullWritable\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "sc = spark.sparkContext\n",
    "hdfs_project_path = \"hdfs:///Projects/genre_classifier_2/\"\n",
    "dataset_path = hdfs_project_path + \"Spectrograms/\"\n",
    "output_base_folder = dataset_path + \"tfrecords/\"\n",
    "\n",
    "# Testing\n",
    "input_folder = dataset_path + \"testing\"\n",
    "output_folder = output_base_folder + \"testing\"\n",
    "write_tf_records(sc, input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
