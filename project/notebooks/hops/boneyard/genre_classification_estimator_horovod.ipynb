{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def wrapper(learning_rate, dropout):\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    from hops import tensorboard\n",
    "    from hops import hdfs\n",
    "    from tensorflow.contrib.data import Dataset, Iterator\n",
    "    import horovod.tensorflow as hvd\n",
    "    import random\n",
    "    \n",
    "    # Our music genre labels\n",
    "    label_dict = {\n",
    "        'Classical': 0,\n",
    "        'Techno': 1,\n",
    "        'Pop': 2,\n",
    "        'HipHop': 3,\n",
    "        'Metal': 4,\n",
    "        'Rock': 5\n",
    "    }\n",
    "\n",
    "    # Parameters\n",
    "    n_classes = len(label_dict)\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 100\n",
    "    num_steps = 100000 / batch_size    # Size of data set\n",
    "    display_step = 10\n",
    "    dropout = 0.75  # Dropout, probability to keep units\n",
    "    \n",
    "    dataset_path = hdfs.project_path() + \"Spectrograms/\"\n",
    "        \n",
    "    # CNN methods\n",
    "    \n",
    "    def conv_net(x, n_classes, dropout, reuse, is_training):\n",
    "        # Define a scope for reusing the variables\n",
    "        with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "            # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "            # Reshape to match picture format [Height x Width x Channel]\n",
    "            # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "            x = tf.reshape(x, shape=[-1, 128, 128, 1])\n",
    "\n",
    "            # Convolution Layer with 32 filters and a kernel size of 5\n",
    "            conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "            # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "            conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "            # Convolution Layer with 32 filters and a kernel size of 5\n",
    "            conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "            # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "            conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "            # Flatten the data to a 1-D vector for the fully connected layer\n",
    "            fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "            # Fully connected layer (in contrib folder for now)\n",
    "            fc1 = tf.layers.dense(fc1, 1024)\n",
    "            # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "            fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "            # Output layer, class prediction\n",
    "            out = tf.layers.dense(fc1, n_classes)\n",
    "            # Because 'softmax_cross_entropy_with_logits' already apply softmax,\n",
    "            # we only apply softmax to testing network\n",
    "            out = tf.nn.softmax(out) if not is_training else out\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    # Adapted from the medium guy\n",
    "    def conv_net2(x, n_classes, dropout, reuse, is_training):\n",
    "        with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "            x = tf.reshape(x, shape=[-1, 128, 128, 1])\n",
    "            conv = tf.layers.conv2d(x, 64, 2, activation=tf.nn.relu)\n",
    "            conv = tf.layers.max_pooling2d(conv, 2, 2)\n",
    "            conv = tf.layers.conv2d(conv, 128, 2, activation=tf.nn.relu)\n",
    "            conv = tf.layers.max_pooling2d(conv, 2, 2 )\n",
    "            conv = tf.layers.conv2d(conv, 256, 2, activation=tf.nn.relu)\n",
    "            conv = tf.layers.max_pooling2d(conv, 2, 2)\n",
    "            conv = tf.layers.conv2d(conv, 512, 2, activation=tf.nn.relu)\n",
    "            conv = tf.layers.max_pooling2d(conv, 2, 2)\n",
    "            conv = tf.contrib.layers.flatten(conv)\n",
    "            conv = tf.layers.dense(conv, 1024)\n",
    "            conv = tf.layers.dropout(conv, rate=dropout, training=is_training)\n",
    "            out = tf.layers.dense(conv, n_classes)\n",
    "            out = tf.nn.softmax(out) if not is_training else out\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    # Define the model function (following TF Estimator Template)\n",
    "    def model_fn(features, labels, mode, params):\n",
    "        \n",
    "        # Build the neural network\n",
    "        # Because Dropout have different behavior at training and prediction time, we\n",
    "        # need to create 2 distinct computation graphs that still share the same weights.\n",
    "        logits_train = conv_net2(features, n_classes, dropout, reuse=False, is_training=True)\n",
    "        logits_test = conv_net2(features, n_classes, dropout, reuse=True, is_training=False)\n",
    "\n",
    "        # Predictions\n",
    "        pred_classes = tf.argmax(logits_test, axis=1)\n",
    "        pred_probas = tf.nn.softmax(logits_test)\n",
    "\n",
    "        # If prediction mode, early return\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "\n",
    "        # Define loss\n",
    "        # https://datascience.stackexchange.com/a/22458\n",
    "        #loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "        loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_train, labels=labels))\n",
    "\n",
    "        # Horovod changes below here. \n",
    "        # Adapted from: https://gist.github.com/alsrgv/34a32f30292f4e2c1fa29ec0d65dea26\n",
    "        # Horovod: Scale learning rate by the number of workers\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate * hvd.size())\n",
    "        # Horovod: Add Horovod distributed optimizer\n",
    "        optimizer = hvd.DistributedOptimizer(optimizer)            \n",
    "        train_op = optimizer.minimize(loss=loss_op, global_step=tf.train.get_global_step())\n",
    "\n",
    "        # Evaluate the accuracy of the model\n",
    "        #hdfs.log(\"Labels shape, pred_classes shape: \" + str(tf.shape(labels)) + \", \" + str(tf.shape(pred_classes)))\n",
    "        # https://stackoverflow.com/a/46414395\n",
    "        #acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "        acc_op = tf.metrics.accuracy(labels=tf.argmax(labels,1), predictions=pred_classes)\n",
    "\n",
    "        # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "        # the different ops for training, evaluating, ...\n",
    "        estim_specs = tf.estimator.EstimatorSpec(\n",
    "          mode=mode,\n",
    "          predictions=pred_classes,\n",
    "          loss=loss_op,\n",
    "          train_op=train_op,\n",
    "          eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "        return estim_specs\n",
    "\n",
    "    \n",
    "    def data_input_fn(foldername, batch_size=128, shuffle=False, repeat=None):\n",
    "        \n",
    "        def input_parser(img_path, label_value):\n",
    "            label = tf.one_hot(label_value, n_classes)\n",
    "            img_file = tf.read_file(img_path)\n",
    "            img_decoded = tf.cast(tf.image.decode_image(img_file, channels=1), tf.float32)\n",
    "            return img_decoded, label\n",
    "    \n",
    "    \n",
    "        def load_and_shuffle_data(data_folder):\n",
    "            hdfs.log(\"Loading and shuffling data from folder \" + dataset_path + data_folder)\n",
    "            data = []\n",
    "            for genre_name in label_dict:\n",
    "                filenames_path = dataset_path + data_folder + \"/\" + genre_name\n",
    "                for filename_path in tf.gfile.Glob(filenames_path + \"/*.png\"):\n",
    "                    filename = filename_path.split(\"/\")[len(filename_path.split(\"/\")) - 1]\n",
    "                    genre = filename.split(\"_\")[0]\n",
    "\n",
    "                    # if we can't extract the label from the image we should not train on it\n",
    "                    if genre not in label_dict:\n",
    "                        continue;\n",
    "\n",
    "                    label_val = int(label_dict.get(genre))\n",
    "                    data.append((filename_path, label_val))\n",
    "\n",
    "            random.shuffle(data)\n",
    "            image_paths = [x[0] for x in data]\n",
    "            labels = [x[1] for x in data]\n",
    "\n",
    "            return image_paths, labels\n",
    "\n",
    "    \n",
    "        def _input_fn():\n",
    "            images, labels = load_and_shuffle_data(foldername)\n",
    "            hdfs.log(\"Loaded data from folder, size: \" + foldername + \", \" + str(len(images)))\n",
    "            data_set = Dataset.from_tensor_slices((images, labels))\n",
    "            data_set = data_set.map(input_parser)\n",
    "            \n",
    "            if shuffle:\n",
    "                data_set = data_set.shuffle(buffer_size=128)\n",
    "            \n",
    "            data_set = data_set.batch(batch_size)\n",
    "            data_set = data_set.repeat(repeat)\n",
    "            \n",
    "            iterator = data_set.make_one_shot_iterator()\n",
    "            features, labels = iterator.get_next()\n",
    "            \n",
    "            return features, labels    \n",
    "        \n",
    "        return _input_fn\n",
    "\n",
    "    # Horovod: Initialize\n",
    "    hvd.init()\n",
    "    \n",
    "    # TODO Do we need to split the input data among the workers manually?\n",
    "    # As they do for mnist:  mnist = learn.datasets.mnist.read_data_sets('MNIST-data-%d' % hvd.rank())\n",
    "\n",
    "    # Horovod: pin GPU to be used to process local rank (one GPU per process)\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
    "    \n",
    "    # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n",
    "    model_dir = tensorboard.logdir() if hvd.rank() == 0 else None\n",
    "    \n",
    "    # Horovod: BroadcastGlobalVariablesHook broadcasts initial variable states from\n",
    "    # rank 0 to all other processes. This is necessary to ensure consistent\n",
    "    # initialization of all workers when training is started with random weights or\n",
    "    # restored from a checkpoint.\n",
    "    bcast_hook = hvd.BroadcastGlobalVariablesHook(0)\n",
    "    \n",
    "    # Horovod: reduce number of training steps inversely proportional to the number of workers.\n",
    "    num_steps = num_steps // hvd.size()\n",
    "    \n",
    "    run_config = tf.contrib.learn.RunConfig(\n",
    "        model_dir=model_dir,\n",
    "        save_checkpoints_steps=10,\n",
    "        save_summary_steps=5,\n",
    "        log_step_count_steps=10)\n",
    "\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "        learning_rate=learning_rate, dropout_rate=dropout)\n",
    "\n",
    "    summary_hook = tf.train.SummarySaverHook(\n",
    "      save_steps = run_config.save_summary_steps,\n",
    "      scaffold= tf.train.Scaffold(),\n",
    "      summary_op=tf.summary.merge_all())\n",
    "\n",
    "    mnist_estimator = tf.estimator.Estimator(\n",
    "        model_fn=model_fn,\n",
    "        config=run_config,\n",
    "        params=hparams\n",
    "        )\n",
    "\n",
    "    train_input_fn = data_input_fn(\"training\", batch_size=batch_size)\n",
    "    eval_input_fn = data_input_fn(\"validation\", batch_size=batch_size)\n",
    "\n",
    "    experiment = tf.contrib.learn.Experiment(\n",
    "        mnist_estimator,\n",
    "        train_input_fn=train_input_fn,\n",
    "        eval_input_fn=eval_input_fn,\n",
    "        train_steps=num_steps,\n",
    "        min_eval_frequency=5,\n",
    "        eval_hooks=[summary_hook]\n",
    "        )\n",
    "\n",
    "    experiment.train_and_evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from hops import util\n",
    "\n",
    "#Define dict for hyperparameters\n",
    "args_dict = {'learning_rate': [0.005], 'dropout': [0.75]}\n",
    "\n",
    "# Generate a grid for the given hyperparameters\n",
    "args_dict_grid = util.grid_params(args_dict)\n",
    "\n",
    "print(args_dict_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from hops import tflauncher\n",
    "\n",
    "tensorboard_hdfs_logdir = tflauncher.launch(spark, wrapper, args_dict_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
