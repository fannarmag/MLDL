{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Music classification by genre - local notebook\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.data import Dataset, Iterator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our music genre labels\n",
    "label_dict = {\n",
    "    'Classical': 0,\n",
    "    'Electronic': 1,\n",
    "    'Pop': 2,\n",
    "    'HipHop': 3,\n",
    "    'Metal': 4,\n",
    "    'Rock': 5\n",
    "}\n",
    "\n",
    "NUM_CLASSES = label_dict.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_parser(img_path, label_value):\n",
    "    label = tf.one_hot(label_value, NUM_CLASSES)\n",
    "    img_file = tf.read_file(img_path)\n",
    "    img_decoded = tf.image.decode_image(img_file, channels=1)\n",
    "\n",
    "    return img_decoded, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_shuffle_data():\n",
    "    data = []\n",
    "    for file in glob.glob(\"data/*/**.png\"):\n",
    "        # extract label\n",
    "        filename = file.split(\"/\")[len(file.split(\"/\")) - 1]\n",
    "        genre = filename.split(\"_\")[0]\n",
    "\n",
    "        # if we can't extract the label from the image we should not train on it\n",
    "        if genre not in label_dict:\n",
    "            continue;\n",
    "        \n",
    "        label_val = label_dict.get(genre)\n",
    "        data.append((file, label_val))\n",
    "    \n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_and_shuffle_data()\n",
    "images = [x[0] for x in data]\n",
    "labels = [x[1] for x in data]\n",
    "\n",
    "# create TensorFlow Dataset objects\n",
    "tf_data = Dataset.from_tensor_slices((images, labels))\n",
    "tf_data = tf_data.map(input_parser)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate our data, plot first element\n",
    "# create TensorFlow Iterator object\n",
    "iterator = Iterator.from_structure(tf_data.output_types, tf_data.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# create two initialization ops to switch between the datasets\n",
    "training_init_op = iterator.make_initializer(tf_data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # initialize the iterator on the data\n",
    "    sess.run(training_init_op)\n",
    "    elem = sess.run(next_element)\n",
    "    print(\"label:\" + str(elem[1]))\n",
    "    two_d_image = elem[0].reshape(128, 128)\n",
    "    plt.imshow(two_d_image, cmap='Greys')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = tf_data.batch(10)\n",
    "iterator = batch_data.make_one_shot_iterator()\n",
    "next_image, next_label = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print (next_label.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_with_hyperparameters(dropout, starter_learning_rate):\n",
    "    # 1. Define Variables and Placeholders\n",
    "    X = tf.placeholder(tf.float32, [None, 128, 128, 1]) # The first dimension (None) will index the images\n",
    "    Y_ = tf.placeholder(tf.float32, [None, 6])          # Correct answers\n",
    "    \n",
    "    # placeholder for probability of keeping a node during dropout\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Convolutional layers\n",
    "    CL1W = tf.Variable(tf.truncated_normal([5, 5, 1, 4], stddev=0.1))\n",
    "    CL1B = tf.Variable(tf.constant(0.1, shape=[4]))\n",
    "    CL2W = tf.Variable(tf.truncated_normal([5, 5, 4, 8], stddev=0.1))\n",
    "    CL2B = tf.Variable(tf.constant(0.1, shape=[8]))\n",
    "    CL3W = tf.Variable(tf.truncated_normal([4, 4, 8, 12], stddev=0.1))\n",
    "    CL3B = tf.Variable(tf.constant(0.1, shape=[12]))\n",
    "    \n",
    "    # Fully connected layer\n",
    "    # We should have 12 32*32 images, one for each feature\n",
    "    FCW = tf.Variable(tf.truncated_normal([32 * 32 * 12, 200], stddev=0.1))\n",
    "    FCB = tf.Variable(tf.zeros([200]))\n",
    "    \n",
    "    # Read out layer\n",
    "    ROW = tf.Variable(tf.truncated_normal([200, 6], stddev=0.1))\n",
    "    ROB = tf.Variable(tf.zeros([6]))\n",
    "    \n",
    "    # Convolutional and ReLU layers\n",
    "    CL1 = tf.nn.conv2d(X, CL1W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    Y1 = tf.nn.relu(CL1 + CL1B)\n",
    "    Y1d = tf.nn.dropout(Y1, pkeep)\n",
    "    CL2 = tf.nn.conv2d(Y1d, CL2W, strides=[1, 2, 2, 1], padding='SAME')\n",
    "    Y2 = tf.nn.relu(CL2 + CL2B)\n",
    "    Y2d = tf.nn.dropout(Y2, pkeep)\n",
    "    CL3 = tf.nn.conv2d(Y2d, CL3W, strides=[1, 2, 2, 1], padding='SAME')\n",
    "    Y3 = tf.nn.relu(CL3 + CL3B)\n",
    "    Y3d = tf.nn.dropout(Y3, pkeep)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    # Reshape to vector\n",
    "    # Y3.shape = (?, 32, 32, 12)\n",
    "    #Y3RS = tf.reshape(Y3, [-1, 128 * 128 * 1])\n",
    "    Y3RS = tf.reshape(Y3d, [-1, 32 * 32 * 12])\n",
    "    Y4 = tf.nn.relu(tf.matmul(Y3RS, FCW) + FCB)\n",
    "    \n",
    "    # Read out layer\n",
    "    Ylogits = tf.matmul(Y4, ROW) + ROB\n",
    "    Y = tf.nn.softmax(Ylogits)\n",
    "    \n",
    "    # 3. Define the loss function\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # 4. Define the accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # 5. Define an optimizer - Gradient Descent Optimizer or Adam Optimizer\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    # decay every 100 steps with a base of 0.96\n",
    "    decay_steps = 100\n",
    "    decay_rate = 0.96\n",
    "    # If the argument staircase is True, then global_step / decay_steps is an integer division \n",
    "    # and the decayed learning rate follows a staircase function.\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        starter_learning_rate, \n",
    "        global_step, \n",
    "        decay_steps, \n",
    "        decay_rate, \n",
    "        staircase=True)\n",
    "\n",
    "    # Note: Passing global_step to minimize() will increment it at each step.\n",
    "    # train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy, global_step=global_step)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy, global_step=global_step)\n",
    "    \n",
    "    # initialize\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    \n",
    "    def training_step(i, update_test_data, update_train_data, dropout):\n",
    "\n",
    "        print \"\\r\", i,\n",
    "        ####### actual learning \n",
    "        batch_data = tf_data.batch(100)\n",
    "        iterator = batch_data.make_one_shot_iterator()\n",
    "        batch_X, batch_Y = iterator.get_next()\n",
    "        \n",
    "        batch_X = sess.run(batch_X)\n",
    "        batch_Y = sess.run(batch_Y)\n",
    "        \n",
    "        #print(batch_X.shape)\n",
    "        # the backpropagation training step\n",
    "        sess.run(train_step, feed_dict={X: batch_X, Y_: batch_Y, pkeep: dropout})\n",
    "\n",
    "        ####### evaluating model performance for printing purposes\n",
    "        # evaluation used to later visualize how well you did at a particular time in the training\n",
    "        train_a = []\n",
    "        train_c = []\n",
    "        test_a = []\n",
    "        test_c = []\n",
    "        if update_train_data:\n",
    "            a, c = sess.run([accuracy, cross_entropy], feed_dict=\n",
    "                            {X: batch_X, Y_: batch_Y, pkeep: 1})\n",
    "            train_a.append(a)\n",
    "            train_c.append(c)\n",
    "\n",
    "            \n",
    "        # TODO : Here we should input our test data\n",
    "        # X: mnist.test.images, Y_: mnist.test.labels,\n",
    "        # Now we are testing on the same data as we are training on\n",
    "        if update_test_data:\n",
    "            a, c = sess.run([accuracy, cross_entropy], feed_dict=\n",
    "                            {X: batch_X, Y_: batch_Y,  pkeep: 1})\n",
    "            test_a.append(a)\n",
    "            test_c.append(c)\n",
    "\n",
    "        return (train_a, train_c, test_a, test_c)\n",
    "    \n",
    "    # 6. Train and test the model, store the accuracy and loss per iteration\n",
    "    train_a = []\n",
    "    train_c = []\n",
    "    test_a = []\n",
    "    test_c = []\n",
    "\n",
    "    training_iter = 100\n",
    "    epoch_size = 10\n",
    "    for i in range(training_iter):\n",
    "        test = False\n",
    "        if i % epoch_size == 0:\n",
    "            test = True\n",
    "        a, c, ta, tc = training_step(i, test, test, dropout)\n",
    "        train_a += a\n",
    "        train_c += c\n",
    "        test_a += ta\n",
    "        test_c += tc\n",
    "        \n",
    "    # 7. Plot and visualise the accuracy and loss\n",
    "    # accuracy training vs testing dataset\n",
    "    plt.plot(train_a)\n",
    "    plt.plot(test_a)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # loss training vs testing dataset\n",
    "    plt.plot(train_c)\n",
    "    plt.plot(test_c)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Zoom in on the tail of the plots\n",
    "    zoom_point = 50\n",
    "    x_range = range(zoom_point,training_iter/epoch_size)\n",
    "    plt.plot(x_range, train_a[zoom_point:])\n",
    "    plt.plot(x_range, test_a[zoom_point:])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_c[zoom_point:])\n",
    "    plt.plot(test_c[zoom_point:])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # TODO : Should be able to get test data here\n",
    "    # X: mnist.test.images, Y_: mnist.test.labels\n",
    "    batch_data = tf_data.batch(100)\n",
    "    iterator = batch_data.make_one_shot_iterator()\n",
    "    batch_X, batch_Y = iterator.get_next()\n",
    "        \n",
    "    batch_X = sess.run(batch_X)\n",
    "    batch_Y = sess.run(batch_Y)\n",
    "    \n",
    "    # Accuracy\n",
    "    print(sess.run(accuracy, feed_dict={X: batch_X, Y_: batch_Y, pkeep: 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_with_hyperparameters(0.5, 0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
